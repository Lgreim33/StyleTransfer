{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torchvision.models as models\n",
    "import os\n",
    "from torch.utils.data import random_split\n",
    "from PIL import Image\n",
    "import time\n",
    "import torchvision.transforms as transforms\n",
    "from pytorch_msssim import ssim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sobel SSIM Loss Definition #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the Gradient magnitude of the passed image and returns it, does so by performing a directional sobel convolution in the x and y direction and then using those as inputs for the gradient magnitude function\n",
    "def sobel_filter(image):\n",
    "\n",
    "    # setup the kernals for direction sobel\n",
    "    sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "    sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # resize to cover all channels of the passed image\n",
    "    channels = image.size(1)\n",
    "    sobel_x = sobel_x.repeat(channels, 1, 1, 1).to(image.device)\n",
    "    sobel_y = sobel_y.repeat(channels, 1, 1, 1).to(image.device)\n",
    "\n",
    "    # apply sobel in x and y direction\n",
    "    edges_x = F.conv2d(image, sobel_x, padding=1, groups=channels)\n",
    "    edges_y = F.conv2d(image, sobel_y, padding=1, groups=channels)\n",
    "\n",
    "    edges = torch.sqrt(edges_x ** 2 + edges_y ** 2 + 1e-6)\n",
    "    \n",
    "    # normalize the gradient magnitude and return\n",
    "    edges = (edges - edges.min()) / (edges.max() - edges.min() + 1e-6) \n",
    "    return edges\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class to gice to the dataloader to load our style and content images into the model\n",
    "class StyleContentDataset(Dataset):\n",
    "    def __init__(self, content_dir, style_dir):\n",
    "        # Get the path to all image files in the content folder\n",
    "        self.content_images = [os.path.join(content_dir, img) for img in os.listdir(content_dir) if img.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        \n",
    "\n",
    "        # To my utter shagrin, the style images are in subfolders, so we have to go through all of them to get each one's path\n",
    "        self.style_images = []\n",
    "        for subdir in os.listdir(style_dir):\n",
    "            subdir_path = os.path.join(style_dir, subdir)\n",
    "            if os.path.isdir(subdir_path):\n",
    "                style_images_in_subdir = [os.path.join(subdir_path, img) for img in os.listdir(subdir_path) if img.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "                self.style_images.extend(style_images_in_subdir)\n",
    "        \n",
    "        # transforms to perform on the image when being loaded into the model\n",
    "        self.transform = transforms.Compose([\n",
    "\n",
    "            transforms.Resize((512, 512)),\n",
    "            # random crop prevents the model from overfitting\n",
    "            transforms.RandomCrop((256,256)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    # I'm not completely sure what the best practice is, because technically its two datasets of different sizes in one loader, but this shouldn't matter for now because we're using a very small subset \n",
    "    def __len__(self):\n",
    "        return max(len(self.content_images), len(self.style_images))\n",
    "\n",
    "    # Return the ith image pair\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the corresponding image path\n",
    "        content_img_path = self.content_images[idx % len(self.content_images)]\n",
    "        style_img_path = self.style_images[idx % len(self.style_images)]\n",
    "        \n",
    "        # Retreive the image as a PIL\n",
    "        content_image = Image.open(content_img_path).convert(\"RGB\")\n",
    "        style_image = Image.open(style_img_path).convert(\"RGB\")\n",
    "\n",
    "        # Transform the image to be returned\n",
    "        content_image = self.transform(content_image)\n",
    "        style_image = self.transform(style_image)\n",
    "\n",
    "        return content_image,style_image\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sets #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123403\n"
     ]
    }
   ],
   "source": [
    "# Dataset paths, relative to this folder\n",
    "coco_path = \"DataSets/unlabeled2017/\"\n",
    "wikiart_path = \"DataSets/wikiart/\"\n",
    "\n",
    "# Create the custom dataset\n",
    "Dataset = StyleContentDataset(coco_path,wikiart_path)\n",
    "\n",
    "# Split is pretty arbitrary, as we wont use nearly all of the images in either the test or train split\n",
    "train_dataset ,test_dataset= random_split(Dataset,[0.8,0.2])\n",
    "\n",
    "# Create the train and test dataloaders\n",
    "train_loader = DataLoader(train_dataset,batch_size=1,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaIN #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The code for AdaIN was borrowed from this individual, who re-wrote the original code in python (was Lua)\n",
    "https://github.com/naoto0804/pytorch-AdaIN/blob/master/function.py\n",
    "\n",
    "Original: https://github.com/xunhuang1995/AdaIN-style/blob/master/lib/AdaptiveInstanceNormalization.lua\n",
    "\n",
    "When I say \"Original,\" I'm reffering to the code that the authors of the paper that this is based of wrote for their paper\n",
    "'''\n",
    "\n",
    "\n",
    "def calc_mean_std(feat, eps=1e-5):\n",
    "    # eps is a small value added to the variance to avoid divide-by-zero.\n",
    "    size = feat.size()\n",
    "    assert (len(size) == 4)\n",
    "    N, C = size[:2]\n",
    "    feat_var = feat.view(N, C, -1).var(dim=2) + eps\n",
    "    feat_std = feat_var.sqrt().view(N, C, 1, 1)\n",
    "    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n",
    "    return feat_mean, feat_std\n",
    "\n",
    "def mean_variance_norm(feat):\n",
    "    size = feat.size()\n",
    "    mean, std = calc_mean_std(feat)\n",
    "    normalized_feat = (feat - mean.expand(size)) / std.expand(size)\n",
    "    return normalized_feat\n",
    "\n",
    "\n",
    "\n",
    "def AdaIn(content_feat, style_feat):\n",
    "    assert (content_feat.size()[:2] == style_feat.size()[:2])\n",
    "    size = content_feat.size()\n",
    "    style_mean, style_std = calc_mean_std(style_feat)\n",
    "    content_mean, content_std = calc_mean_std(content_feat)\n",
    "\n",
    "    normalized_feat = (content_feat - content_mean.expand(\n",
    "        size)) / content_std.expand(size)\n",
    "    return normalized_feat * style_std.expand(size) + style_mean.expand(size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definitons #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBAM #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    This code for CBAM was borrowed from this github:https://github.com/Jongchan/attention-module/blob/c06383c514ab0032d044cc6fcd8c8207ea222ea7/MODELS/cbam.py#L84\n",
    "\n",
    "    It's the official implementation from the researchers who first proposed it\n",
    "'''\n",
    "\n",
    "\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=False, bias=False):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.out_channels = out_planes\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
    "        self.relu = nn.ReLU() if relu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class ChannelGate(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n",
    "        super(ChannelGate, self).__init__()\n",
    "        self.gate_channels = gate_channels\n",
    "        self.mlp = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n",
    "            )\n",
    "        self.pool_types = pool_types\n",
    "    def forward(self, x):\n",
    "        channel_att_sum = None\n",
    "        for pool_type in self.pool_types:\n",
    "            if pool_type=='avg':\n",
    "                avg_pool = F.avg_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( avg_pool )\n",
    "            elif pool_type=='max':\n",
    "                max_pool = F.max_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( max_pool )\n",
    "            elif pool_type=='lp':\n",
    "                lp_pool = F.lp_pool2d( x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( lp_pool )\n",
    "            elif pool_type=='lse':\n",
    "                # LSE pool only\n",
    "                lse_pool = logsumexp_2d(x)\n",
    "                channel_att_raw = self.mlp( lse_pool )\n",
    "\n",
    "            if channel_att_sum is None:\n",
    "                channel_att_sum = channel_att_raw\n",
    "            else:\n",
    "                channel_att_sum = channel_att_sum + channel_att_raw\n",
    "\n",
    "        scale = F.sigmoid( channel_att_sum ).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
    "        return x * scale\n",
    "\n",
    "def logsumexp_2d(tensor):\n",
    "    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)\n",
    "    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)\n",
    "    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n",
    "    return outputs\n",
    "\n",
    "class ChannelPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n",
    "\n",
    "class SpatialGate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialGate, self).__init__()\n",
    "        kernel_size = 7\n",
    "        self.compress = ChannelPool()\n",
    "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n",
    "    def forward(self, x):\n",
    "        x_compress = self.compress(x)\n",
    "        x_out = self.spatial(x_compress)\n",
    "        scale = F.sigmoid(x_out) # broadcasting\n",
    "        return x * scale\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n",
    "        self.no_spatial=no_spatial\n",
    "        if not no_spatial:\n",
    "            self.SpatialGate = SpatialGate()\n",
    "    def forward(self, x):\n",
    "        x_out = self.ChannelGate(x)\n",
    "        if not self.no_spatial:\n",
    "            x_out = self.SpatialGate(x_out)\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Decoder #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Get the pretrained vgg19 model\n",
    "        vgg = models.vgg19(pretrained=True).features\n",
    "\n",
    "        # Define layers to extract features at specific layers\n",
    "        self.model = nn.Sequential(*[vgg[i] for i in range(len(vgg))])  \n",
    "        \n",
    "        # Freeze weights so we don't change anything we didn't mean to \n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # These need to be calculated seperatly, as for some loss calculation tasks we care about the higher level feature maps\n",
    "        self.relu1_1 = self.model[:2]\n",
    "        self.relu2_1 = self.model[2:7]\n",
    "        self.relu3_1 = self.model[7:12]\n",
    "        self.relu4_1 = self.model[12:21]\n",
    "        self.relu5_1 = self.model[21:28]\n",
    "\n",
    "    # Generate feature maps, X is the input image, returns relu1_1 - relu4_1 feature maps\n",
    "    def forward(self, x):\n",
    "\n",
    "        feat1_1 = self.relu1_1(x)\n",
    "        feat2_1 = self.relu2_1(feat1_1)\n",
    "        feat3_1 = self.relu3_1(feat2_1)\n",
    "        feat4_1 = self.relu4_1(feat3_1)\n",
    "\n",
    "        return feat1_1,feat2_1,feat3_1,feat4_1\n",
    "\n",
    "# Custom decoder model, takes the processed feature map and reconstructs it back to the original image space as it goes along\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Decoder mirrors the encoder, upsampling as it passes through each layer\n",
    "        self.deconv_relu4_1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1)  \n",
    "        )\n",
    "        self.deconv_relu3_1 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1) \n",
    "        )\n",
    "        self.deconv_relu2_1 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)  \n",
    "        )\n",
    "        self.deconv_relu1_1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), \n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),  \n",
    "            nn.Sigmoid()  \n",
    "        )\n",
    "\n",
    "    def forward(self, feat4_1):\n",
    "    \n",
    "        #Reverse the feature maps through the decoder, output is the finalized image\n",
    "    \n",
    "        x = self.deconv_relu4_1(feat4_1)  \n",
    "        x = self.deconv_relu3_1(x)        \n",
    "        x = self.deconv_relu2_1(x)       \n",
    "        x = self.deconv_relu1_1(x)        \n",
    "        output = self.final(x)          \n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Put the models together into a single model\n",
    "class StyleTransferModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StyleTransferModel, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder().eval()\n",
    "        self.cbam = CBAM(512)\n",
    "        self.decoder = Decoder() \n",
    "\n",
    "        #alpha can be altered to adjust style application strength post training (0-1)\n",
    "        self.alpha = 1.0\n",
    "\n",
    "\n",
    "    # Process the content and style images\n",
    "    def forward(self, content, style):\n",
    "\n",
    "        # Pass content and style through encoder to get  feature maps\n",
    "        content_feat1_1, content_feat2_1, content_feat3_1,content_feat4_1 = self.encoder(content)\n",
    "        style_feat1_1, style_feat2_1, style_feat3_1, style_feat4_1 = self.encoder(style)\n",
    "\n",
    "        # Place these in lists so we can access them easily later\n",
    "        content_feats = [content_feat1_1, content_feat2_1, content_feat3_1,content_feat4_1]\n",
    "        style_feats = [style_feat1_1, style_feat2_1, style_feat3_1, style_feat4_1]\n",
    "\n",
    "        # Apply CBAM, add the out put back into the original for the skip connection\n",
    "        attention4_1 = self.cbam(content_feat4_1)\n",
    "\n",
    "        # Skip connection\n",
    "        attention_boosted_4_1 = content_feat4_1+attention4_1\n",
    "\n",
    "        # Perform adaptive instance normalization with to fuse style into content\n",
    "        fused_feat4_1 = AdaIn(attention_boosted_4_1,style_feat4_1)\n",
    "\n",
    "        # Just scales the degree of which the style is applied, if one it remains the same\n",
    "        fused_feat4_1 = self.alpha * fused_feat4_1 + (1 - self.alpha) * content_feat4_1\n",
    "    \n",
    "\n",
    "        # decode the image\n",
    "        generated_image = self.decoder(fused_feat4_1)\n",
    "        \n",
    "        return generated_image,style_feats,content_feats\n",
    "\n",
    "    \n",
    "# Custom Loss Class, will be used for both model cases, but the experiment with no ssim will set ssim lambda to 0\n",
    "\n",
    "'''\n",
    "The code for the style and content loss was borrowed from this individual, who re-wrote the original code in python (was Lua)\n",
    "https://github.com/naoto0804/pytorch-AdaIN/blob/master/net.py\n",
    "\n",
    "Original: https://github.com/xunhuang1995/AdaIN-style/blob/master/lib/ContentLossModule.lua\n",
    "\n",
    "Specifically, the code for calc_content_loss and calc_style_loss were used\n",
    "'''\n",
    "\n",
    "class StyleTransferLoss(nn.Module):\n",
    "    def __init__(self, vgg_encoder, lambda_c=1, lambda_s=4, lambda_ssim=7):\n",
    "\n",
    "        super(StyleTransferLoss, self).__init__()\n",
    "        self.vgg_encoder = vgg_encoder \n",
    "        self.lambda_c = lambda_c\n",
    "        self.lambda_s = lambda_s\n",
    "        self.lambda_ssim = lambda_ssim\n",
    "        self.mse_loss = nn.MSELoss()   \n",
    "\n",
    "    def calc_content_loss(self, input, target):\n",
    "        # MSE loss for content preservation\n",
    "        assert input.size() == target.size(), \"Content size mismatch!\"\n",
    "        return self.mse_loss(input, target)\n",
    "\n",
    "    def calc_style_loss(self, input, target):\n",
    "        # Style loss based on mean and variance\n",
    "        assert input.size() == target.size(), \"Style size mismatch!\"\n",
    "        input_mean, input_std = calc_mean_std(input)\n",
    "        target_mean, target_std = calc_mean_std(target)\n",
    "        return self.mse_loss(input_mean, target_mean) + self.mse_loss(input_std, target_std)\n",
    "\n",
    "    # Actual loss calcualtion, takes the generated image, as well as the content image, and the feature maps for content and style\n",
    "    def forward(self, generated_image, content,content_feats, style_feats):\n",
    "\n",
    "        # encode the generated image for loss calculation\n",
    "        gen_feats = self.vgg_encoder(generated_image)\n",
    "\n",
    "        \n",
    "        # calculate content loss (last feature map for structure preservation)\n",
    "        content_loss = self.calc_content_loss(gen_feats[-1], content_feats[-1])\n",
    "        \n",
    "        # calculate style loss (mean and variance alignment for each layer)\n",
    "        style_loss = 0\n",
    "        for gen_feat, style_feat in zip(gen_feats, style_feats):\n",
    "            style_loss += self.calc_style_loss(gen_feat, style_feat)\n",
    "\n",
    "        # Sobel-SSIM edge loss\n",
    "        sobel_gen = sobel_filter(generated_image)\n",
    "        sobel_content = sobel_filter(content)\n",
    "\n",
    "        edge_loss = 1 - ssim(sobel_gen, sobel_content, data_range=1.0, size_average=True)\n",
    "\n",
    "\n",
    "        # Combine the losses with weights\n",
    "        total_loss = (self.lambda_c * content_loss +\n",
    "                      self.lambda_s * style_loss +\n",
    "                      self.lambda_ssim * edge_loss)\n",
    "\n",
    "        return total_loss,self.lambda_ssim*edge_loss,self.lambda_s*style_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# trains the model for a set number of epcochs\n",
    "def train_models(model,criterion,dataloader,optimizer,scheduler,device):\n",
    "\n",
    "    epochs = 100\n",
    "\n",
    "    model.train()\n",
    "    count = 0\n",
    "\n",
    "\n",
    "    # Run for X number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        count = 0\n",
    "        start_time = time.time()\n",
    "        for content, style in dataloader:\n",
    "\n",
    "            # Send images to whatever device you passed\n",
    "            content = content.to(device)\n",
    "            style = style.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get the generated image and the feature maps from the model\n",
    "            generated_image,style_feat,content_feat = model(content,style)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss,_,_ = criterion(generated_image,content,content_feat,style_feat)\n",
    "          \n",
    "            # Backpropagate and update weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            count +=1 \n",
    "\n",
    "            # Used for visualizing where in the epoch the model is at\n",
    "            print(count,end='\\r')\n",
    "\n",
    "            # Only look at the first X number of training examples, it should be random, as the training loader shuffles the data on initilization\n",
    "            if count == 7000:\n",
    "                break\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "        # For my sanity, prints out the epoch and how long it took to run\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "        print(f\"Time: {(time.time()-start_time)/60} Minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lgrei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lgrei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge3.462749481201172 Style1.2553887367248535\n",
      "Epoch 1, Loss: 7.896272659301758\n",
      "Time: 6.449324035644532 Minutes\n",
      "Edge5.683238983154297 Style1.3784103393554688\n",
      "Epoch 2, Loss: 8.13656997680664\n",
      "Time: 6.3909780263900755 Minutes\n",
      "Edge5.302268981933594 Style1.831329584121704\n",
      "Epoch 3, Loss: 9.792922019958496\n",
      "Time: 6.7086461186409 Minutes\n",
      "Edge3.971255302429199 Style1.3627915382385254\n",
      "Epoch 4, Loss: 10.371631622314453\n",
      "Time: 6.896966830889384 Minutes\n",
      "Edge1.0859711170196533 Style1.0053917169570923\n",
      "Epoch 5, Loss: 2.5375499725341797\n",
      "Time: 7.092501862843831 Minutes\n",
      "Edge3.378678798675537 Style2.2697253227233887\n",
      "Epoch 6, Loss: 8.118995666503906\n",
      "Time: 6.970884378751119 Minutes\n",
      "Edge3.911449670791626 Style1.3485547304153442\n",
      "Epoch 7, Loss: 6.542032241821289\n",
      "Time: 6.365428133805593 Minutes\n",
      "Edge3.0122227668762207 Style1.3001965284347534\n",
      "Epoch 8, Loss: 7.9517717361450195\n",
      "Time: 6.559653882185618 Minutes\n",
      "Edge1.9433660507202148 Style2.0005435943603516\n",
      "Epoch 9, Loss: 5.625954627990723\n",
      "Time: 6.63205056587855 Minutes\n",
      "Edge3.6623969078063965 Style0.5219078063964844\n",
      "Epoch 10, Loss: 6.52540397644043\n",
      "Time: 6.775356952349345 Minutes\n",
      "Edge1.9509341716766357 Style1.2537384033203125\n",
      "Epoch 11, Loss: 4.540556907653809\n",
      "Time: 6.7607864260673525 Minutes\n",
      "Edge1.8850719928741455 Style1.073936939239502\n",
      "Epoch 12, Loss: 3.7276759147644043\n",
      "Time: 6.52180312871933 Minutes\n",
      "Edge2.273580551147461 Style0.677855908870697\n",
      "Epoch 13, Loss: 3.911161184310913\n",
      "Time: 6.348676156997681 Minutes\n",
      "Edge1.61197829246521 Style1.2460358142852783\n",
      "Epoch 14, Loss: 4.047793388366699\n",
      "Time: 6.214141070842743 Minutes\n",
      "Edge4.616098403930664 Style1.0740724802017212\n",
      "Epoch 15, Loss: 7.3057379722595215\n",
      "Time: 7.073219668865204 Minutes\n",
      "4538\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lgrei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (107327830 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge2.536205291748047 Style1.5355401039123535\n",
      "Epoch 16, Loss: 5.844758033752441\n",
      "Time: 7.0176416794459024 Minutes\n",
      "Edge2.2578887939453125 Style2.6250014305114746\n",
      "Epoch 17, Loss: 6.7155232429504395\n",
      "Time: 6.880134491125743 Minutes\n",
      "Edge1.209285855293274 Style1.6690489053726196\n",
      "Epoch 18, Loss: 4.039950370788574\n",
      "Time: 6.833343199888865 Minutes\n",
      "Edge2.3843870162963867 Style0.5817639827728271\n",
      "Epoch 19, Loss: 3.7558188438415527\n",
      "Time: 6.662921484311422 Minutes\n",
      "Edge1.369013786315918 Style2.329885482788086\n",
      "Epoch 20, Loss: 4.833584308624268\n",
      "Time: 6.8915875593821205 Minutes\n",
      "Edge3.2283499240875244 Style0.8348174095153809\n",
      "Epoch 21, Loss: 5.1935224533081055\n",
      "Time: 6.806363924344381 Minutes\n",
      "Edge1.7092539072036743 Style0.6939399242401123\n",
      "Epoch 22, Loss: 3.359801769256592\n",
      "Time: 6.779898472627004 Minutes\n",
      "Edge3.225130558013916 Style3.4501068592071533\n",
      "Epoch 23, Loss: 11.82862663269043\n",
      "Time: 6.812688195705414 Minutes\n",
      "2667\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lgrei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (99962094 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge3.001750946044922 Style1.3115540742874146\n",
      "Epoch 24, Loss: 7.428616046905518\n",
      "Time: 6.827441120147705 Minutes\n",
      "Edge2.2055487632751465 Style0.7052072882652283\n",
      "Epoch 25, Loss: 4.067363739013672\n",
      "Time: 6.782912838459015 Minutes\n",
      "Edge3.9290685653686523 Style4.993216037750244\n",
      "Epoch 26, Loss: 11.827619552612305\n",
      "Time: 6.246721239884694 Minutes\n",
      "Edge2.090888023376465 Style0.9396463632583618\n",
      "Epoch 27, Loss: 4.236567974090576\n",
      "Time: 6.485826575756073 Minutes\n",
      "Edge2.7354319095611572 Style1.0887556076049805\n",
      "Epoch 28, Loss: 5.957897186279297\n",
      "Time: 7.359858926137289 Minutes\n",
      "Edge3.264051675796509 Style0.7513430714607239\n",
      "Epoch 29, Loss: 5.108750820159912\n",
      "Time: 7.615512855847677 Minutes\n",
      "Edge3.154996395111084 Style0.9105139374732971\n",
      "Epoch 30, Loss: 8.340774536132812\n",
      "Time: 7.361159328619639 Minutes\n",
      "Edge1.2222042083740234 Style1.1892342567443848\n",
      "Epoch 31, Loss: 3.1345396041870117\n",
      "Time: 7.017178348700205 Minutes\n",
      "Edge2.870455741882324 Style0.9920600652694702\n",
      "Epoch 32, Loss: 5.512818336486816\n",
      "Time: 6.8750208735466005 Minutes\n",
      "Edge4.956734657287598 Style0.7332617044448853\n",
      "Epoch 33, Loss: 6.980644702911377\n",
      "Time: 6.924102679888407 Minutes\n",
      "Edge3.5514822006225586 Style0.8728458285331726\n",
      "Epoch 34, Loss: 7.186885833740234\n",
      "Time: 6.824199807643891 Minutes\n",
      "Edge1.2934229373931885 Style1.4874980449676514\n",
      "Epoch 35, Loss: 3.5095133781433105\n",
      "Time: 6.790325923760732 Minutes\n",
      "Edge2.9444215297698975 Style1.7006922960281372\n",
      "Epoch 36, Loss: 6.696572303771973\n",
      "Time: 6.881147090593974 Minutes\n",
      "Edge3.143810272216797 Style1.3144140243530273\n",
      "Epoch 37, Loss: 5.465707778930664\n",
      "Time: 6.7884263634681705 Minutes\n",
      "Edge4.690492153167725 Style2.7574968338012695\n",
      "Epoch 38, Loss: 10.494160652160645\n",
      "Time: 6.4654449383417765 Minutes\n",
      "Edge2.8733396530151367 Style2.0469226837158203\n",
      "Epoch 39, Loss: 6.902594566345215\n",
      "Time: 6.493761583169301 Minutes\n",
      "Edge1.9336745738983154 Style1.2584792375564575\n",
      "Epoch 40, Loss: 4.625903129577637\n",
      "Time: 7.177945518493653 Minutes\n",
      "Edge2.290630340576172 Style0.6235381960868835\n",
      "Epoch 41, Loss: 6.161776542663574\n",
      "Time: 7.066758386294047 Minutes\n",
      "Edge2.5270721912384033 Style5.740478515625\n",
      "Epoch 42, Loss: 10.953207969665527\n",
      "Time: 7.122701068719228 Minutes\n",
      "Edge3.066082715988159 Style1.2463579177856445\n",
      "Epoch 43, Loss: 6.37740421295166\n",
      "Time: 6.531233398119609 Minutes\n",
      "Edge2.5149998664855957 Style1.0197813510894775\n",
      "Epoch 44, Loss: 6.277946472167969\n",
      "Time: 5.615379516283671 Minutes\n",
      "Edge2.456580638885498 Style1.684201955795288\n",
      "Epoch 45, Loss: 6.431670188903809\n",
      "Time: 5.562954926490784 Minutes\n",
      "Edge2.9415481090545654 Style1.2842649221420288\n",
      "Epoch 46, Loss: 6.127449035644531\n",
      "Time: 5.5326387484868365 Minutes\n",
      "Edge1.7762272357940674 Style2.4485762119293213\n",
      "Epoch 47, Loss: 5.895073890686035\n",
      "Time: 5.551941684881846 Minutes\n",
      "Edge3.4016971588134766 Style1.1938469409942627\n",
      "Epoch 48, Loss: 7.376657962799072\n",
      "Time: 5.508602476119995 Minutes\n",
      "Edge3.2014360427856445 Style1.3862695693969727\n",
      "Epoch 49, Loss: 6.151762008666992\n",
      "Time: 5.543294513225556 Minutes\n",
      "Edge2.127835750579834 Style0.9270776510238647\n",
      "Epoch 50, Loss: 4.465282440185547\n",
      "Time: 6.083673262596131 Minutes\n",
      "Edge2.893671989440918 Style1.0098719596862793\n",
      "Epoch 51, Loss: 7.304279327392578\n",
      "Time: 8.46124515136083 Minutes\n",
      "Edge1.985371708869934 Style1.6650365591049194\n",
      "Epoch 52, Loss: 4.963853359222412\n",
      "Time: 8.550573925177256 Minutes\n",
      "Edge4.230832099914551 Style1.2471485137939453\n",
      "Epoch 53, Loss: 8.624134063720703\n",
      "Time: 9.247712914148966 Minutes\n",
      "Edge4.373489856719971 Style0.5819603800773621\n",
      "Epoch 54, Loss: 10.01407241821289\n",
      "Time: 9.21733318567276 Minutes\n",
      "Edge2.3771581649780273 Style0.6467247605323792\n",
      "Epoch 55, Loss: 4.027137756347656\n",
      "Time: 8.32075670560201 Minutes\n",
      "Edge2.9078452587127686 Style0.582701563835144\n",
      "Epoch 56, Loss: 4.617358684539795\n",
      "Time: 7.136966653664907 Minutes\n",
      "Edge3.1456851959228516 Style0.980584979057312\n",
      "Epoch 57, Loss: 6.304535388946533\n",
      "Time: 7.176759660243988 Minutes\n",
      "Edge2.9254441261291504 Style0.8311770558357239\n",
      "Epoch 58, Loss: 5.147171974182129\n",
      "Time: 7.966025241216024 Minutes\n",
      "Edge3.040152072906494 Style0.9089111685752869\n",
      "Epoch 59, Loss: 6.200545310974121\n",
      "Time: 7.328313338756561 Minutes\n",
      "Edge1.9090657234191895 Style0.851656436920166\n",
      "Epoch 60, Loss: 4.159891128540039\n",
      "Time: 7.2801628947258 Minutes\n",
      "4855\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# send model to be trained\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[43mtrain_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 22\u001b[0m, in \u001b[0;36mtrain_models\u001b[1;34m(model, criterion, dataloader, optimizer, scheduler, device)\u001b[0m\n\u001b[0;32m     19\u001b[0m generated_image,style_feat,content_feat \u001b[38;5;241m=\u001b[39m model(content,style)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m loss,edge_l,style_l \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcontent_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstyle_feat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Backpropagate and update weights\u001b[39;00m\n\u001b[0;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\lgrei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lgrei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 170\u001b[0m, in \u001b[0;36mStyleTransferLoss.forward\u001b[1;34m(self, generated_image, content, content_feats, style_feats)\u001b[0m\n\u001b[0;32m    167\u001b[0m     style_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_style_loss(gen_feat, style_feat)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m#sobel-SSIM edge loss\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m sobel_gen \u001b[38;5;241m=\u001b[39m \u001b[43msobel_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m sobel_content \u001b[38;5;241m=\u001b[39m sobel_filter(content)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m#edge_loss = ssim_loss(sobel_gen, sobel_content)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m, in \u001b[0;36msobel_filter\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      4\u001b[0m sobel_y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      6\u001b[0m channels \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Number of channels\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m sobel_x \u001b[38;5;241m=\u001b[39m \u001b[43msobel_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m sobel_y \u001b[38;5;241m=\u001b[39m sobel_y\u001b[38;5;241m.\u001b[39mrepeat(channels, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(image\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     10\u001b[0m edges_x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mconv2d(image, sobel_x, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, groups\u001b[38;5;241m=\u001b[39mchannels)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = StyleTransferModel()\n",
    "\n",
    "#make sure we dont update the encoder\n",
    "for param in model.encoder.model.parameters():\n",
    "    assert param.requires_grad == False\n",
    "\n",
    "criterion = StyleTransferLoss(model.encoder)\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "# Training options\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# send model to be trained\n",
    "train_models(model,criterion,train_loader,optimizer,scheduler,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "\n",
    "# Visual Evaluation of the model\n",
    "with torch.no_grad(): \n",
    "    i = 0\n",
    "    for content,style in test_loader:\n",
    "        stylized_image = model(content.to(device),style.to(device))\n",
    "        i +=1\n",
    "\n",
    "        # Quick and dirty way to look at the ith inferenced image, not great but its nice for quickly evaluating model performance post training\n",
    "        if (i <= 3):\n",
    "            continue\n",
    "        # display as a PIL\n",
    "        stylized_image = transforms.ToPILImage()(stylized_image[0][0])\n",
    "        content_im = transforms.ToPILImage()(style[0])\n",
    "        style_im = transforms.ToPILImage()(content[0])\n",
    "\n",
    "        \n",
    "        stylized_image.show()\n",
    "        content_im.show()\n",
    "        style_im.show()\n",
    "        \n",
    "        break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the trained model \n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, 'newSSIMTEST.pth')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
